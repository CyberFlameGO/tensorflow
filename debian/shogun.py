#!/usr/bin/python3.6
# TF-Shogun: Distribution Friendly Light-Weight Build for TensorFlow.
#
#Copyright: 2018 Mo Zhou <lumin@debian.org>
#License: Expat
# Permission is hereby granted, free of charge, to any person obtaining a
# copy of this software and associated documentation files (the "Software"),
# to deal in the Software without restriction, including without limitation
# the rights to use, copy, modify, merge, publish, distribute, sublicense,
# and/or sell copies of the Software, and to permit persons to whom the
# Software is furnished to do so, subject to the following conditions:
# .
# The above copyright notice and this permission notice shall be included
# in all copies or substantial portions of the Software.
# .
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
# IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
# CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, 
# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE 
# SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

'''
TF-Shogun
=========

Distribution friendly light-weight build for TensorFlow.
Especially written for Debian GNU/Linux.

Shogun needs the bazel dumps from bazelQuery.sh .
And it tries to produce similar result to that from bazel.

References
----------

1. CMake build, tensorflow/contrib/cmake
2. Makefile build, tensorflow/contrib/makefile
3. TensorFlow's native Bazel build.
4. For extra compiler definitions .e.g TENSORFLOW_USE_JEMALLOC please lookup
   tensorflow/core/platform/default/build_config.bzl
5. ArchLinux PKGBUILD
   https://git.archlinux.org/svntogit/community.git/tree/trunk/PKGBUILD?h=packages/tensorflow
6. Gentoo ebuild
   https://packages.gentoo.org/packages/sci-libs/tensorflow
'''
# FIXME: libtensorflow.so and libtensorflow_cc.so are still not linked
# against mkl-dnn, xsmm .
# FIXME: how to use blas/mkl to improve speed?

from typing import *
import sys
import re
import os
import argparse
import json
import glob
import subprocess
from pprint import pprint
from ninja_syntax import Writer


# FIXME: don't forget to bump soversion when upstream version changes!
tf_soversion = '1.10'


def ninjaCommonHeader(cursor: Writer, ag: Any) -> None:
    '''
    Writes a common header to the ninja file. ag is parsed arguments.
    '''
    cursor.comment('-- start common ninja header --')
    cursor.comment(f'Note, this ninja file was automatically generated by {__file__}')
    cursor.newline()
    cursor.comment('-- compiling tools --')
    cursor.newline()
    cursor.variable('CXX', 'g++')
    cursor.variable('PROTOC', '/usr/bin/protoc')
    cursor.variable('PROTO_TEXT', f'./proto_text')
    cursor.variable('SHOGUN_EXTRA', '') # used for adding specific flags for a specific target
    cursor.newline()
    cursor.comment('-- compiler flags --')
    cursor.newline()
    cursor.variable('CPPFLAGS', '-D_FORTIFY_SOURCE=2 ' + str(os.getenv('CPPFLAGS', '')))
    cursor.variable('CXXFLAGS', '-std=c++14 -O2 -pipe -fPIC -gsplit-dwarf -DNDEBUG'
        + ' -fstack-protector-strong -w ' + str(os.getenv('CXXFLAGS', '')))
    cursor.variable('LDFLAGS', '-Wl,-z,relro -Wl,-z,now ' + str(os.getenv('LDFLAGS', '')))
    cursor.variable('INCLUDES', '-I. -I./debian/embedded/eigen3 -I./third_party/eigen3/'
            + ' -I/usr/include/gemmlowp -I/usr/include/llvm-c-7'
            + ' -I/usr/include/llvm-7 -Ithird_party/toolchains/gpus/cuda/')
    cursor.variable('LIBS', '-lpthread -lprotobuf -lnsync -lnsync_cpp -ldouble-conversion'
	+ ' -ldl -lm -lz -lre2 -ljpeg -lpng -lsqlite3 -llmdb -lsnappy -lgif -lLLVM-7')
    cursor.newline()
    cursor.comment('-- compiling rules-- ')
    cursor.rule('rule_PROTOC', f'$PROTOC $in --cpp_out . $SHOGUN_EXTRA')
    cursor.rule('rule_PROTOC_GRPC', f'$PROTOC --grpc_out . --cpp_out . --plugin protoc-gen-grpc=/usr/bin/grpc_cpp_plugin $in')
    cursor.rule('rule_PROTO_TEXT', f'$PROTO_TEXT tensorflow/core tensorflow/core tensorflow/tools/proto_text/placeholder.txt $in')
    cursor.rule('rule_CXX_OBJ', f'$CXX $CPPFLAGS $CXXFLAGS $INCLUDES $SHOGUN_EXTRA -c $in -o $out')
    cursor.rule('rule_CXX_EXEC', f'$CXX $CPPFLAGS $CXXFLAGS $INCLUDES $LDFLAGS $LIBS $SHOGUN_EXTRA $in -o $out')
    cursor.rule('rule_CXX_SHLIB', f'$CXX -shared -fPIC $CPPFLAGS $CXXFLAGS $INCLUDES $LDFLAGS $LIBS $SHOGUN_EXTRA $in -o $out')
    cursor.rule('rule_CC_OP_GEN', f'LD_LIBRARY_PATH=. ./$in $out $cc_op_gen_internal tensorflow/core/api_def/base_api')
    cursor.rule('COPY', f'cp $in $out')
    cursor.newline()
    cursor.comment('-- end common ninja header --')
    cursor.newline()


def cyan(s: str) -> str:
    return f'\033[1;36m{s}\033[0;m'

def yellow(s: str) -> str:
    return f'\033[1;33m{s}\033[0;m'

def red(s: str) -> str:
    return f'\033[1;31m{s}\033[0;m'


def eGrep(pat: Any, lst: List[str]) -> (List[str], List[str]):
    '''
    Just like grep -E
    pat could be str or List[str]
    '''
    match, unmatch = [], []
    if not any((isinstance(pat, str), isinstance(pat, list))):
        raise TypeError("Undefined argument type")
    pat = pat if isinstance(pat, list) else [pat]
    for item in lst:
        if any(re.match(x, item) for x in pat):
            match.append(item)
        else:
            unmatch.append(item)
    return match, unmatch


def eComplain(lst: List[str]) -> None:
    '''
    Print warning message if detected unprocessed files.
    '''
    if not lst: return
    for x in lst:
        print(yellow('? HowToDealWith'), x)
    raise Exception(red(f'{len(Rall)} files to be generated left unresolved'))


def eGlob(pat: str, *, filt: List[str] = [], vfilt: List[str] = []) -> List[str]:
    '''
    Extended version of glob.glob, which globs file and apply
    filters and reverse filters on the result.
    '''
    globs = glob.glob(pat, recursive=True)
    for f in filt:
        globs, _ = eGrep(f, globs)
    for vf in vfilt:
        _, globs = eGrep(f, globs)
    return globs


def eUniq(pat: str, rep: str, lst: List[str]) -> List[str]:
    '''
    lst -> re.sub(pat, rep, ...) -> uniq -> return
    '''
    return list(sorted(set([re.sub(pat, rep, x) for x in lst])))


def getDpkgArchitecture(query: str) -> str:
    '''
    dpkg-architecture -qQUERY
    '''
    # XXX: I wish we don't need to use this function.
    result = subprocess.Popen(['dpkg-architecture', f'-q{query}'],
             stdout=subprocess.PIPE).communicate()[0].decode().strip()
    return result


def bazelPreprocess(srclist: List[str]) -> List[str]:
    '''
    1. Filter out external dependencies from bazel dependency dump.
    2. Mangle file path.
    3. Report the depending libraries.
    '''
    deplist, retlist = set([]), []
    for src in srclist:
        if re.match('^@(\w*).*', src):
            # It's an external dependency
            deplist.update(re.match('^@(\w*).*', src).groups())
        elif re.match('^..third_party.*', src):
            pass # ignore
        else:
            # it's an tensorflow source
            retlist.append(re.sub('^//', '', re.sub(':', '/', src)))
    print(cyan('Required Depends:'))
    pprint(deplist, indent=4, compact=True)
    print('Globbed', cyan(f'{len(srclist)}'), 'source files')
    return retlist


###############################################################################
###############################################################################


def shogunProtoText(argv):
    '''
    Build a binary ELF executable named proto_text, which generates
    XXX.pb_text{.cc,.h,-impl.h} files from a given XXX.proto file.
    This binary file is for one-time use.

    Input: bazelDump
    Output: proto_text
    '''
    ag = argparse.ArgumentParser()
    ag.add_argument('-i', type=str, required=True,
            help='list of source files')
    ag.add_argument('-g', type=str, required=True,
            help='list of generated files')
    ag.add_argument('-o', type=str, default='proto_text.ninja',
            help='where to write the ninja file')
    ag = ag.parse_args(argv)
    print(red('Argument Dump:'))
    pprint(vars(ag))

    # (0) read bazel dump and apply hardcoded filters
    srclist = bazelPreprocess([l.strip() for l in open(ag.i, 'r').readlines()])
    genlist = bazelPreprocess([l.strip() for l in open(ag.g, 'r').readlines()])
    srclist.extend(genlist)
    _, srclist = eGrep('.*.h$', srclist) # nothing to do
    _, srclist = eGrep('^third_party', srclist) # no third_party stuff
    _, srclist = eGrep('.*windows/.*', srclist) # no windoge source
    _, srclist = eGrep('.*.proto$', srclist) # nothing to do 

    # (1) Instantiate ninja writer and generate targets
    cursor = Writer(open(ag.o, 'w'))
    ninjaCommonHeader(cursor, ag)
    cclist, srclist = eGrep('.*.cc$', srclist)
    objlist = []
    for cc in cclist:
        obj = cc.replace('.cc', '.o')
        cursor.build(obj, 'rule_CXX_OBJ', cc)
        objlist.append(obj)

    # (2) link objects into ELF: proto_text
    cursor.build(f'proto_text', 'rule_CXX_EXEC', objlist,
            variables={'LIBS': '-lpthread -lprotobuf -ldouble-conversion'})

    # (.) finish
    cursor.close()
    eComplain(srclist)


def shogunTFLib_framework(argv):
    '''
    Build libtensorflow_framework.so, and a byproduct for tf_cc_op_gen.

    Input: bazelDump
    Output: libtensorflow_framework.so
    '''
    ag = argparse.ArgumentParser()
    ag.add_argument('-i', type=str, required=True,
            help='list of source files')
    ag.add_argument('-g', type=str, required=True,
            help='list of generated files')
    ag.add_argument('-o', type=str, default='libtensorflow_framework.ninja',
            help='where to write the ninja file', )
    ag.add_argument('-H', type=str, default='libtensorflow_framework.hdrs',
            help='a list of header files')
    ag = ag.parse_args(argv)
    print(red('Argument Dump:'))
    pprint(vars(ag))

    # (0) read bazel dump and apply hardcoded filters
    srclist = bazelPreprocess([l.strip() for l in open(ag.i, 'r').readlines()])
    genlist = bazelPreprocess([l.strip() for l in open(ag.g, 'r').readlines()])
    srclist.extend(genlist)
    _, srclist = eGrep('.*proto_text.gen_proto_text_functions.cc', srclist)
    _, srclist = eGrep('^third_party', srclist)
    _, srclist = eGrep('.*/windows/.*', srclist) # no windoge source.
    _, srclist = eGrep('.*.proto$', srclist) # nothing to do
    Rheaders, srclist = eGrep('.*.h$', srclist)  # nothing to do

    # (1) Initialize ninja file and generate object targets
    cursor = Writer(open(ag.o, 'w'))
    ninjaCommonHeader(cursor, ag)
    cclist, srclist = eGrep('.*.cc', srclist)
    objlist = []
    for cc in cclist:
        obj = cc.replace('.cc', '.o')
        cursor.build(obj, 'rule_CXX_OBJ', cc)
        objlist.append(obj)

    # (2) link the shared object
    libs = '''farmhash highwayhash snappy gif double-conversion
              z protobuf jpeg nsync nsync_cpp pthread
              '''.split()
    libs = ' '.join(f'-l{x}' for x in libs)
    extra = f'''-Wl,--soname=libtensorflow_framework.so.{tf_soversion}
                -Wl,--version-script tensorflow/tf_framework_version_script.lds
                -fvisibility=hidden
             '''.split()
    extra = ' '.join(x for x in extra)
    cursor.build('libtensorflow_framework.so', 'rule_CXX_SHLIB', inputs=objlist,
        variables={'LIBS': libs, 'SHOGUN_EXTRA': extra})

    # (3) link the byproduct for tf_cc_op_gen
    libtfccopgen, _ = eGrep(['.*core/kernels.*', '.*core/ops.*'], objlist)
    cursor.build('tfccopgen.so', 'rule_CXX_SHLIB', libtfccopgen,
            variables={'LIBS': libs})

    # done
    cursor.close()
    eComplain(srclist)


def shogunTFLib(argv):
    '''
    Build libtensorflow.so or libtensorflow_cc.so
    '''
    ag = argparse.ArgumentParser()
    ag.add_argument('-i', type=str, required=True,
            help='list of source files')
    ag.add_argument('-g', type=str, required=True,
            help='list of generated files')
    ag.add_argument('-o', type=str, required=True,
            help='where to write the ninja file')
    ag.add_argument('-O', type=str, required=True,
            help='the file name shared object')
    ag.add_argument('-H', type=str, required=True,
            help='where to put the headers list')
    ag = ag.parse_args(argv)
    print(red('Argument Dump:'))
    pprint(vars(ag))

    # (0) read bazel dump and apply hard-coded filters
    srclist = bazelPreprocess([l.strip() for l in open(ag.i, 'r').readlines()])
    genlist = bazelPreprocess([l.strip() for l in open(ag.g, 'r').readlines()])
    extra_srcs = ['debian/embedded/fft/fftsg.c']
    srclist = list(set(srclist + genlist + extra_srcs))
    _, srclist = eGrep('^third_party', srclist)
    _, srclist = eGrep('.*/windows/.*', srclist) # no windoge source.
    _, srclist = eGrep('.*.cu.cc$', srclist) # no CUDA file for CPU-only build
    _, srclist = eGrep('.*.pbtxt$', srclist) # not for us
    _, srclist = eGrep('.*platform/cloud.*', srclist) # SSL 1.1.1 broke it.
    _, srclist = eGrep('.*platform/s3.*', srclist) # we don't have https://github.com/aws/aws-sdk-cpp
    _, srclist = eGrep('.*_main.cc$', srclist) # don't include any main function.
    _, srclist = eGrep('.*test.*', srclist) # don't include any test
    _, srclist = eGrep('.*cc_op_gen.*', srclist) # don't include cc_op_gen.
    _, srclist = eGrep('.*gen_proto_text_functions.cc', srclist) # not for this library
    _, srclist = eGrep('.*tensorflow.contrib.cloud.*', srclist) # it wants GoogleAuthProvider etc.
    _, srclist = eGrep('.*gcs_config_ops.cc', srclist) # it wants GcsFileSystem
    Rheaders, srclist = eGrep('.*.h$', srclist) # nothing to do
    _, srclist = eGrep('.*.proto$', srclist) # nothing to do

    if getDpkgArchitecture('DEB_HOST_ARCH') != 'amd64':
        # they FTBFS on non-amd64 arches
        _, srclist = eGrep('.*/core/debug/.*', srclist)
        _, srclist = eGrep('.*debug_ops.*', srclist)

    # (1) Instantiate ninja writer and compile objects
    exception_eigen_avoid_std_array = [
        'sparse_tensor_dense_matmul_op',
        'conv_grad_ops_3d',
        'adjust_contrast_op'
        ]
    cursor = Writer(open(ag.o, 'w'))
    ninjaCommonHeader(cursor, ag)
    cclist, srclist = eGrep('.*.cc', srclist)
    objlist = []
    for cc in cclist:
        obj = re.sub('.c[c]?$', '.o', cc)
        variables = {}
        if any(x in cc for x in exception_eigen_avoid_std_array):
            variables = {'SHOGUN_EXTRA': '-DEIGEN_AVOID_STL_ARRAY'}
        cursor.build(obj, 'rule_CXX_OBJ', cc, variables=variables)
        objlist.append(obj)

    # (2) link the final shared object
    libs = '''pthread protobuf nsync nsync_cpp double-conversion jpeg png
              gif highwayhash farmhash jsoncpp sqlite3 re2 curl lmdb snappy
              dl z m LLVM-7 grpc++
              '''.split()
    libs = ' '.join(f'-l{x}' for x in libs)
    extra = f'''-Wl,--soname={ag.O}.{tf_soversion} -fvisibility=hidden
             '''.split()
    extra.append('-Wl,--version-script tensorflow/c/version_script.lds')
    extra = ' '.join(x for x in extra)
    cursor.build(ag.O, 'rule_CXX_SHLIB', objlist,
            variables={'LIBS': libs, 'SHOGUN_EXTRA': extra})

    # (3) write down the related header files
    with open(ag.H, 'w') as f:
        f.writelines([x + '\n' for x in eUniq('', '', Rheaders)])

    # (.) finish
    cursor.close()
    eComplain(Rall)


def shogunGenerator(argv):
    '''
    Generic File Generator. It determines how to generate the given list
    of files by checking every file name extention.
    '''
    ag = argparse.ArgumentParser()
    ag.add_argument('-g', type=str, required=True,
            help='list of generated files',)
    ag.add_argument('-o', type=str, required=True,
            help='where to write the ninja file')
    ag = ag.parse_args(argv)
    print(red('Argument Dump:'))
    pprint(vars(ag))

    # (0.1) Read the list and apply hard-coded filters.
    Rall = bazelPreprocess([l.strip() for l in open(ag.g, 'r').readlines()])

    # (0.1) Instantiate ninja writer
    cursor = Writer(open(ag.o, 'w'))
    ninjaCommonHeader(cursor, ag)

    # (1.1) Collect protobuf-grpc stuff and generate targets
    Rgrpc_pb_all, Rall = eGrep(['.*.grpc.pb.cc', '.*.grpc.pb.h'], Rall)
    Lgrpc_proto = eUniq('.grpc.pb.cc$', '.proto', Rgrpc_pb_all)
    Lgrpc_proto = eUniq('.grpc.pb.h$', '.proto', Lgrpc_proto)
    for proto in Lgrpc_proto:
        Tcc = re.sub('.proto$', '.grpc.pb.cc', proto)
        Th  = re.sub('.proto$', '.grpc.pb.h', proto)
        if getDpkgArchitecture('DEB_HOST_ARCH') == 'amd64':
            cursor.build([Tcc, Th], 'rule_PROTOC_GRPC', proto)

    # (1.2) Collect protobuf stuff and generate targets
    Rpb_all, Rall = eGrep(['.*.pb.cc', '.*.pb.h'], Rall)
    Lproto = eUniq('.pb.cc$', '.proto', Rpb_all)
    Lproto = eUniq('.pb.h$', '.proto', Lproto)
    for proto in Lproto:
        Tcc = re.sub('.proto$', '.pb.cc', proto)
        Th  = re.sub('.proto$', '.pb.h', proto)
        cursor.build([Tcc, Th], 'rule_PROTOC', proto)

    # (1.3) Collect proto-text stuff and generate targets
    Rpb_text_all, Rall = eGrep(['.*.pb_text.h$', '.*.pb_text.cc$',
                          '.*.pb_text-impl.h$'], Rall)
    Lpb_text = eUniq('.pb_text.h$', '.proto', Rpb_text_all)
    Lpb_text = eUniq('.pb_text.cc$', '.proto', Lpb_text)
    Lpb_text = eUniq('.pb_text-impl.h$', '.proto', Lpb_text)
    for proto in Lpb_text:
        Tcc = re.sub('.proto$', '.pb_text.cc', proto)
        Th  = re.sub('.proto$', '.pb_text.h', proto)
        Tih = re.sub('.proto$', '.pb_text-impl.h', proto)
        cursor.build([Tcc, Th, Tih], 'rule_PROTO_TEXT', proto)

    # (2.1) tf_cc_op_gen (YYY_gen_cc)
    Rcc_op_all, Rall = eGrep(['.*/cc/ops/.*.cc', '.*/cc/ops/.*.h'], Rall)
    cursor.build('tensorflow/core/ops/user_ops.cc', 'COPY', 'tensorflow/core/user_ops/fact.cc')
    # - common object
    cclist_extra = ['tensorflow/core/framework/op_gen_lib.cc',
                    'tensorflow/cc/framework/cc_op_gen.cc',
                    'tensorflow/cc/framework/cc_op_gen_main.cc', ]
    objlist = []
    for cc in cclist_extra:
        obj = re.sub('.cc$', '.o', cc)
        cursor.build(obj, 'rule_CXX_OBJ', cc)
        objlist.append(obj)
    # - all ops
    ops = eUniq('.cc$', '', Rcc_op_all)
    ops = eUniq('.h$', '', ops)
    ops = list(set(os.path.basename(x) for x in ops if 'internal' not in x))
    # - build elf executables and generate .cc files
    for op in ops:
        coreopcc = 'tensorflow/core/ops/' + op + '.cc'
        ccopcc   = 'tensorflow/cc/ops/'   + op + '.cc'
        ccoph    = 'tensorflow/cc/ops/'   + op + '.h'
        ccopincc = 'tensorflow/cc/ops/'   + op + '_internal.cc'
        ccopinh  = 'tensorflow/cc/ops/'   + op + '_internal.h'
        cursor.build(f'{op}_gen_cc', 'rule_CXX_EXEC', [coreopcc] + objlist,
            variables={'SHOGUN_EXTRA': '-I. -L. -ltf_ccop'})
        cursor.build([ccoph, ccopcc], 'rule_CC_OP_GEN', '{op}_gen_cc',
                variables={'cc_op_gen_internal': '0' if op != 'sendrecv_ops' else '1'},
                implicit_outputs=[ccopinh, ccopincc])

    # (.) finish
    cursor.close()
    eComplain(Rall)


if __name__ == '__main__':

    # A graceful argparse implementation with argparse subparser requries
    # obscure code. An advantage of the current implementation is that you
    # only need to define a new shogunXXX function and it would be
    # automatically added here.
    try:
        eval(f'shogun{sys.argv[1]}')(sys.argv[2:])
    except (IndexError, NameError) as e:
        print('you must specify one of the following a subcommand:')
        print([k.replace('shogun', '') for (k, v) in locals().items() if k.startswith('shogun')])
        exit(1)
